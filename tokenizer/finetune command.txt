python3 uniform_finetune.py   --model_type baichuan --model_name_or_path /data/zhenghaoz/Baichuan2-13B-Base/ --data ./mydata/data1.json ./mydata/data2.json ./mydata/data3.json ./mydata/data4.json --lora_target_modules W_pack --lora_r 128 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 1 --learning_rate 2e-5 --epochs 2 --report_to none --val_set_size 500 --compute_dtype fp16 --quantization_bit 8 --output_dir ./saved_models/lora1

accelerate config  # This will create a config file on your server
CUDA_VISIBLE_DEVICES=0,4,5,6,7 accelerate launch uniform_finetune.py   --model_type baichuan --model_name_or_path /data/zhenghaoz/Baichuan2-13B-Base/ --data ./mydata/data1.json ./mydata/data2.json ./mydata/data3.json ./mydata/data4.json --lora_target_modules W_pack --lora_r 128 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 1 --learning_rate 2e-5 --epochs 2 --report_to none --val_set_size 500 --compute_dtype fp16 --quantization_bit 4 --output_dir ./saved_models/lora1

CUDA_VISIBLE_DEVICES=4,5,6,7 python3 -m torch.distributed.launch --nproc_per_node 4      --nnodes=1 --node_rank=0  uniform_finetune.py   --model_type baichuan --model_name_or_path /data/zhenghaoz/Baichuan2-13B-Base/ --data ./mydata/data1.json ./mydata/data2.json ./mydata/data3.json ./mydata/data4.json --lora_target_modules W_pack --lora_r 128 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 1 --learning_rate 2e-5 --epochs 2 --report_to none --val_set_size 500 --compute_dtype fp16 --quantization_bit 8 --output_dir ./saved_models/lora1

CUDA_VISIBLE_DEVICES=0,4,5,6,7 torchrun --nnodes 1 --nproc_per_node 5  uniform_finetune.py   --model_type baichuan --model_name_or_path /data/zhenghaoz/Baichuan2-13B-Base/ --data ./mydata/data1.json ./mydata/data2.json ./mydata/data3.json ./mydata/data4.json --lora_target_modules W_pack --lora_r 64 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 1 --learning_rate 2e-5 --epochs 2 --report_to none --val_set_size -1 --compute_dtype fp16 --quantization_bit 4 --output_dir ./saved_models/lora1

CUDA_VISIBLE_DEVICES=0,4,5,6,7 torchrun --nnodes 1 --nproc_per_node 5  uniform_finetune.py   --model_type baichuan --model_name_or_path /data/zhenghaoz/Baichuan2-13B-Base/ --data ./mydata/data2.json ./mydata/data3.json ./mydata/data4.json --lora_target_modules W_pack --lora_r 64 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 1 --learning_rate 2e-5 --epochs 2 --report_to none --val_set_size -1 --compute_dtype fp16 --quantization_bit 4 --output_dir ./saved_models/lora2

CUDA_VISIBLE_DEVICES=0,4,5,6,7 torchrun --nnodes 1 --nproc_per_node 5  uniform_finetune.py   --model_type baichuan --model_name_or_path /data/zhenghaoz/Baichuan2-13B-Base/ --data ./mydata/data1.json  --lora_target_modules W_pack --lora_r 64 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 1 --learning_rate 2e-5 --epochs 2 --report_to none --val_set_size -1 --compute_dtype fp16 --quantization_bit 4 --output_dir ./saved_models/lora1

CUDA_VISIBLE_DEVICES=0,4,5,6,7 torchrun --nnodes 1 --nproc_per_node 5  uniform_finetune.py   --model_type baichuan --model_name_or_path /data/zhenghaoz/Baichuan2-13B-Base/ --data ./mydata/AlpacaCoT-HK_combined_3995_en.json ./mydata/HKNSL_363.json  ./mydata/Quora-HK_combined_1857.json ./mydata/Firefly-HK_combined_4347.json  --lora_target_modules W_pack --lora_r 64 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 1 --learning_rate 2e-5 --epochs 2 --report_to none --val_set_size -1 --compute_dtype fp16 --quantization_bit 4 --output_dir ./saved_models/lora5

CUDA_VISIBLE_DEVICES=0 python  uniform_finetune.py   --model_type baichuan --model_name_or_path /data/zhenghaoz/Baichuan2-13B-Base/ --data ./mydata/AlpacaCoT-HK_combined_52862_zh-cn.json ./mydata/HKNSL_363.json  ./mydata/Quora-HK_combined_1857.json ./mydata/Firefly-HK_combined_4347.json  --lora_target_modules W_pack --lora_r 64 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 1 --learning_rate 2e-5 --epochs 2 --report_to none --val_set_size -1 --compute_dtype fp16 --quantization_bit 4 --output_dir ./saved_models/lora7

CUDA_VISIBLE_DEVICES=0 python  uniform_finetune.py   --model_type llama --model_name_or_path /data/zhenghaoz/llama --data ./mydata/AlpacaCoT-HK_combined_52862_zh-cn.json ./mydata/HKNSL_363.json  ./mydata/Quora-HK_combined_1857.json ./mydata/Firefly-HK_combined_4347.json  --lora_target_modules W_pack --lora_r 64 --lora_alpha 32 --lora_dropout 0.1 --per_gpu_train_batch_size 1 --learning_rate 2e-5 --epochs 2 --report_to none --val_set_size -1 --compute_dtype fp16 --quantization_bit 4 --output_dir ./saved_models/lora8